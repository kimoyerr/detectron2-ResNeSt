{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.engine import DefaultTrainer, default_argument_parser, default_setup, hooks, launch\n",
    "\n",
    "# train data\n",
    "name        = \"TRAIN\"\n",
    "json_file   = \"/home/ec2-user/livecell-dataset/LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_train_mod.json\"\n",
    "image_root  = \"/home/ec2-user/livecell-dataset/LIVECell_dataset_2021/images/livecell_train_val_images/\"\n",
    "# test data\n",
    "name_val        = \"TEST\"\n",
    "json_file_val   = \"/home/ec2-user/livecell-dataset/LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_test_mod.json\"\n",
    "image_root_val  = \"/home/ec2-user/livecell-dataset/LIVECell_dataset_2021/images/livecell_test_images/\"\n",
    "# registr\n",
    "register_coco_instances(name, {}, json_file, image_root)\n",
    "register_coco_instances(name_val, {}, json_file_val, image_root_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = default_argument_parser().parse_args(\"--config-file /home/ec2-user/detectron2-ResNeSt/configs/LiveCell/model/anchor_based/livecell_config.yaml --eval-only OUTPUT_DIR /home/ec2-user/resnest_output MODEL.WEIGHTS http://livecell-dataset.s3.eu-central-1.amazonaws.com/LIVECell_dataset_2021/models/Anchor_based/ALL/LIVECell_anchor_based_model.pth\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "# train data\n",
    "name        = \"TRAIN\"\n",
    "json_file   = \"/home/ec2-user/livecell-dataset/LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_train_mod.json\"\n",
    "image_root  = \"/home/ec2-user/livecell-dataset/LIVECell_dataset_2021/images/livecell_train_val_images/\"\n",
    "# test data\n",
    "name_val        = \"TEST\"\n",
    "json_file_val   = \"/home/ec2-user/livecell-dataset/LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_test_mod.json\"\n",
    "image_root_val  = \"/home/ec2-user/livecell-dataset/LIVECell_dataset_2021/images/livecell_test_images/\"\n",
    "# registr\n",
    "register_coco_instances(name, {}, json_file, image_root)\n",
    "register_coco_instances(name_val, {}, json_file_val, image_root_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.config import get_cfg\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file('/home/ec2-user/detectron2-ResNeSt/configs/LiveCell/model/anchor_based/livecell_config.yaml')\n",
    "cfg.merge_from_list(args.opts)\n",
    "cfg.freeze()\n",
    "default_setup(cfg, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From detectron2-ResNeSt main folder or here: https://github.com/bendangnuksung/detectron2-ResNeSt/blob/resnest/tools/train_net.py\n",
    "class Trainer(DefaultTrainer):\n",
    "    \"\"\"\n",
    "    We use the \"DefaultTrainer\" which contains pre-defined default logic for\n",
    "    standard training workflow. They may not work for you, especially if you\n",
    "    are working on a new research project. In that case you can use the cleaner\n",
    "    \"SimpleTrainer\", or write your own training loop. You can use\n",
    "    \"tools/plain_train_net.py\" as an example.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        \"\"\"\n",
    "        Create evaluator(s) for a given dataset.\n",
    "        This uses the special metadata \"evaluator_type\" associated with each builtin dataset.\n",
    "        For your own dataset, you can simply create an evaluator manually in your\n",
    "        script and do not have to worry about the hacky if-else logic here.\n",
    "        \"\"\"\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        evaluator_list = []\n",
    "        evaluator_type = MetadataCatalog.get(dataset_name).evaluator_type\n",
    "        if evaluator_type in [\"sem_seg\", \"coco_panoptic_seg\"]:\n",
    "            evaluator_list.append(\n",
    "                SemSegEvaluator(\n",
    "                    dataset_name,\n",
    "                    distributed=True,\n",
    "                    num_classes=cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES,\n",
    "                    ignore_label=cfg.MODEL.SEM_SEG_HEAD.IGNORE_VALUE,\n",
    "                    output_dir=output_folder,\n",
    "                )\n",
    "            )\n",
    "        if evaluator_type in [\"coco\", \"coco_panoptic_seg\"]:\n",
    "            evaluator_list.append(COCOEvaluator(dataset_name, cfg, True, output_folder))\n",
    "        if evaluator_type == \"coco_panoptic_seg\":\n",
    "            evaluator_list.append(COCOPanopticEvaluator(dataset_name, output_folder))\n",
    "        elif evaluator_type == \"cityscapes\":\n",
    "            assert (\n",
    "                torch.cuda.device_count() >= comm.get_rank()\n",
    "            ), \"CityscapesEvaluator currently do not work with multiple machines.\"\n",
    "            return CityscapesEvaluator(dataset_name)\n",
    "        elif evaluator_type == \"pascal_voc\":\n",
    "            return PascalVOCDetectionEvaluator(dataset_name)\n",
    "        elif evaluator_type == \"lvis\":\n",
    "            return LVISEvaluator(dataset_name, cfg, True, output_folder)\n",
    "        if len(evaluator_list) == 0:\n",
    "            raise NotImplementedError(\n",
    "                \"no Evaluator for the dataset {} with the type {}\".format(\n",
    "                    dataset_name, evaluator_type\n",
    "                )\n",
    "            )\n",
    "        elif len(evaluator_list) == 1:\n",
    "            return evaluator_list[0]\n",
    "        return DatasetEvaluators(evaluator_list)\n",
    "\n",
    "    @classmethod\n",
    "    def test_with_TTA(cls, cfg, model):\n",
    "        logger = logging.getLogger(\"detectron2.trainer\")\n",
    "        # In the end of training, run an evaluation with TTA\n",
    "        # Only support some R-CNN models.\n",
    "        logger.info(\"Running inference with test-time augmentation ...\")\n",
    "        model = GeneralizedRCNNWithTTA(cfg, model)\n",
    "        evaluators = [\n",
    "            cls.build_evaluator(\n",
    "                cfg, name, output_folder=os.path.join(cfg.OUTPUT_DIR, \"inference_TTA\")\n",
    "            )\n",
    "            for name in cfg.DATASETS.TEST\n",
    "        ]\n",
    "        res = cls.test(cfg, model, evaluators)\n",
    "        res = OrderedDict({k + \"_TTA\": v for k, v in res.items()})\n",
    "        return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Trainer.build_model(cfg)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "\n",
    "DetectionCheckpointer(model, save_dir=cfg.OUTPUT_DIR).resume_or_load(\n",
    "            cfg.MODEL.WEIGHTS, resume=args.resume\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dataset_name = cfg.DATASETS.TEST[0]\n",
    "data_loader = Trainer.build_test_loader(cfg, dataset_name)\n",
    "print(len(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.evaluation import COCOEvaluator # This is teh LiveCell COCOEvaluator that is modified to detect more than 100 instances per image\n",
    "\n",
    "# Build Evaluator\n",
    "evaluator = Trainer.build_evaluator(cfg, dataset_name)\n",
    "data_loader = Trainer.build_test_loader(cfg, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slight modifications of the detectron2 visualizer function from here: https://github.com/facebookresearch/detectron2/blob/7801ac3d595a0663d16c0c9a6a339c77b2ddbdfb/detectron2/utils/visualizer.py#L538\n",
    "import numpy as np\n",
    "from detectron2.structures import BitMasks, Boxes, BoxMode\n",
    "\n",
    "def my_draw_dataset_dict(v, annos):\n",
    "    \"\"\"\n",
    "    Draw annotations/segmentaions in Detectron2 Dataset format.\n",
    "\n",
    "    Args:\n",
    "        v: Detectron2 Visualizer object\n",
    "        dic (dict): annotation/segmentation data of one image, in Detectron2 Dataset format.\n",
    "\n",
    "    Returns:\n",
    "        output (VisImage): image object with visualizations.\n",
    "    \"\"\"\n",
    "\n",
    "    if annos:\n",
    "        if \"segmentation\" in annos[0]:\n",
    "            masks = [x[\"segmentation\"] for x in annos]\n",
    "        else:\n",
    "            masks = None\n",
    "        if \"keypoints\" in annos[0]:\n",
    "            keypts = [x[\"keypoints\"] for x in annos]\n",
    "            keypts = np.array(keypts).reshape(len(annos), -1, 3)\n",
    "        else:\n",
    "            keypts = None\n",
    "\n",
    "        boxes = [BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS) for x in annos]\n",
    "\n",
    "        labels = [x[\"category_id\"] for x in annos]\n",
    "        colors = None\n",
    "        if v._instance_mode == ColorMode.SEGMENTATION and v.metadata.get(\"thing_colors\"):\n",
    "            colors = [\n",
    "                v._jitter([x / 255 for x in v.metadata.thing_colors[c]]) for c in labels\n",
    "            ]\n",
    "        names = v.metadata.get(\"thing_classes\", None)\n",
    "        if names:\n",
    "            labels = [names[i] for i in labels]\n",
    "        labels = [\n",
    "            \"{}\".format(i) + (\"|crowd\" if a.get(\"iscrowd\", 0) else \"\")\n",
    "            for i, a in zip(labels, annos)\n",
    "        ]\n",
    "        v.overlay_instances(\n",
    "            labels=labels, boxes=boxes, masks=masks, keypoints=keypts, assigned_colors=colors\n",
    "        )\n",
    "\n",
    "    # sem_seg = None\n",
    "    # if sem_seg is None and \"sem_seg_file_name\" in dic:\n",
    "    #     sem_seg = cv2.imread(dic[\"sem_seg_file_name\"], cv2.IMREAD_GRAYSCALE)\n",
    "    # if sem_seg is not None:\n",
    "    #     self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\n",
    "    return v.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, cv2, random\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "sel_idx = 18\n",
    "sel_img = evaluator._coco_api.dataset['images'][sel_idx]\n",
    "\n",
    "# Create annotations for the selected image\n",
    "annos = evaluator._coco_api.dataset.get(\"annotations\", None)\n",
    "sel_img_annos = []\n",
    "for a in annos:\n",
    "    if a['image_id']==sel_img['id']:\n",
    "        a['bbox_mode'] = BoxMode.XYWH_ABS\n",
    "        sel_img_annos.append(a)\n",
    "len(sel_img_annos)\n",
    "\n",
    "# Plot image and annotations\n",
    "im = cv2.imread(image_root_val+ sel_img['file_name'])\n",
    "v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata={'thing_classes':['no_cell','cell']},\n",
    "                   scale=0.5, \n",
    "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    )\n",
    "\n",
    "\n",
    "plt.figure(figsize=(50, 100))\n",
    "plt.subplot(1, 2, 1)\n",
    "# Raw image\n",
    "plt.imshow(v.output.get_image()[:, :, ::-1])\n",
    "# Image with annotations\n",
    "out = my_draw_dataset_dict(v, sel_img_annos)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(out.get_image()[:, :, ::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate Model Outputs from the different modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def inference_context(model):\n",
    "    \"\"\"\n",
    "    A context where the model is temporarily changed to eval mode,\n",
    "    and restored to previous mode afterwards.\n",
    "\n",
    "    Args:\n",
    "        model: a torch Module\n",
    "    \"\"\"\n",
    "    training_mode = model.training\n",
    "    model.eval()\n",
    "    yield\n",
    "    model.train(training_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with inference_context(model), torch.no_grad():\n",
    "    for idx, inputs in enumerate(data_loader):\n",
    "        if inputs[0]['image_id']==sel_img['id']:\n",
    "            images = model.preprocess_image(inputs)\n",
    "            features = model.backbone(images.tensor)        \n",
    "            outputs = model(inputs)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.layers import (\n",
    "    Conv2d,\n",
    "    DeformConv,\n",
    "    FrozenBatchNorm2d,\n",
    "    ModulatedDeformConv,\n",
    "    ShapeSpec,\n",
    "    get_norm,\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with inference_context(model), torch.no_grad():\n",
    "    bu_stem = model.backbone.bottom_up.stem(images.tensor)\n",
    "    print(images.tensor.shape)\n",
    "    # Stem: First convolutions\n",
    "    print(bu_stem.shape)\n",
    "\n",
    "    # Architecture of Stem\n",
    "    model.backbone.bottom_up.stem.deep_stem\n",
    "    print(model.backbone.bottom_up.stem.conv1_1)\n",
    "    print(model.backbone.bottom_up.stem.conv1_1.weight.shape)\n",
    "    stem_out_1 = model.backbone.bottom_up.stem.conv1_1(images.tensor)\n",
    "    print(stem_out_1.shape)\n",
    "    stem_out_1 = F.relu_(stem_out_1)\n",
    "    print(stem_out_1.shape)\n",
    "    print(model.backbone.bottom_up.stem.conv1_2.weight.shape)\n",
    "    stem_out_2 = model.backbone.bottom_up.stem.conv1_2(stem_out_1)\n",
    "    stem_out_2 = F.relu_(stem_out_2)\n",
    "    print(stem_out_2.shape)\n",
    "    print(model.backbone.bottom_up.stem.conv1_3.weight.shape)\n",
    "    stem_out_3 = model.backbone.bottom_up.stem.conv1_3(stem_out_1)\n",
    "    stem_out_3 = F.relu_(stem_out_3)\n",
    "    print(stem_out_3.shape)\n",
    "    # Final maxpooling for the stem block\n",
    "    stem_out = F.max_pool2d(stem_out_3, kernel_size=3, stride=2, padding=1)\n",
    "    print(stem_out.shape)\n",
    "\n",
    "    bu_stem = model.backbone.bottom_up.stem(images.tensor)\n",
    "    print(bu_stem.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Blocks in this layer\n",
    "with inference_context(model), torch.no_grad():\n",
    "    print(len(model.backbone.bottom_up.res2))\n",
    "    model.backbone.bottom_up.res2[0]\n",
    "    print(model.backbone.bottom_up.res2(bu_stem).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backbone.bottom_up.res2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backbone.bottom_up.res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.backbone.bottom_up._out_features\n",
    "type(model.backbone.bottom_up)\n",
    "print(model.backbone.bottom_up._out_features)\n",
    "for stage,name in model.backbone.bottom_up.stages_and_names:\n",
    "    print(stage)\n",
    "    print(name)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with inference_context(model), torch.no_grad():\n",
    "    bu = model.backbone.bottom_up(images.tensor)\n",
    "    bu_stem = model.backbone.bottom_up.stem(images.tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.MODEL.RESNETS.STEM_OUT_CHANNELS\n",
    "print(model.backbone.bottom_up._out_feature_strides)\n",
    "print(model.backbone.bottom_up._out_feature_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.backbone.bottom_up.stages_and_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.tensor.shape)\n",
    "# Stem: First convolutions\n",
    "print(bu_stem.shape)\n",
    "\n",
    "# Full Backbone\n",
    "print(bu['res2'].shape)\n",
    "print(bu['res3'].shape)\n",
    "print(bu['res4'].shape)\n",
    "print(bu['res5'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(outputs[0]['instances']))\n",
    "print(len(sel_img_annos))\n",
    "outputs[0]['instances'][0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
